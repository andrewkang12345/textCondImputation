# ğŸ€ Text-Conditioned Trajectory Imputation (Basketball)

âœ¨ **What this does**  
We take basketball tracking data and **hide one playerâ€™s movement** for a few seconds.  
The model then **fills in the missing path**, guided by a **text prompt** you write â€” like:

> *â€œthe center set a screen near the foul line then rolled toward the goal postâ€*

--- 

Basketball tracking data is often incomplete â€” a playerâ€™s movement might be missing because of occlusion, sensor glitches, or data gaps. ğŸ“‰
Traditional imputation methods try to automatically guess these missing paths, but they donâ€™t let analysts inject their own knowledge of the game context. That can lead to predictions that feel unrealistic or detached from what actually happened on the court. ğŸ§

With text-conditioned imputation, we flip the script:
	â€¢	ğŸ“ You (the analyst) describe what happened â€” e.g., â€œthe center set a screen near the foul line then rolled toward the goal post.â€
	â€¢	ğŸ›ï¸ The model uses that prompt to generate a controllable, guided reconstruction of the missing trajectory.
	â€¢	ğŸ§  Instead of a black-box guess, you get imputations that align with your tactical knowledge or what you observed in the video.

This makes imputation not just accurate, but also interactive and analyst-driven â€” a big step beyond one-size-fits-all automation. ğŸš€


<p align="center">
  <img src="test.gif" alt="Prediction vs Ground Truth demo" width="720"/>
</p>

- âœ… **Green path/dots** = Ground truth trajectory of the hidden player  
- ğŸ”µ **Highlighted Blue path/dots** = Modelâ€™s prediction guided by your text  
- ğŸŸ¥ğŸŸ¦ **Other players** = Shown in team colors (from ground truth)  
- â¡ï¸ Offense moves **right â†’ left**

---

## ğŸ“¦ Dataset

We use basketball datasets from ğŸ‘‰ [**UniTraj-pytorch**](https://github.com/colorfulfuture/UniTraj-pytorch).  
This is a preprocessed version of the dataset from [Eric Zhan, et al.](https://github.com/ezhan94/calibratable-style-consistency).

Each file is a pickle containing a NumPy array shaped **(N, T, A, 2):**

- `N` = number of plays  
- `T` = timesteps  
- `A` = players per play  
- `(x, y)` = player coordinates (feet or normalized to `[0,1]`)  

If in feet, we auto-scale by `(x/94, y/50)`.

---

## ğŸ¯ Why text conditioning?

Instead of just guessing, the model uses your **intent**:

- ğŸ“ You write a short prompt describing the movement  
- ğŸ”„ We translate it into **per-timestep grid regions**  
- ğŸ›ï¸ The model gets those as **one-hot vectors (size 40)** alongside player positions and masks  
- ğŸ§  It predicts movements that better align with your description  

---

## ğŸ—ºï¸ Court Grid

- The court is split into an **8 (x) Ã— 5 (y)** grid â†’ **40 regions**  
- Each timestep, the masked agent is assigned to one grid cell (`ID = 0â€“39`)  
- The model conditions on this per-timestep **one-hot(40)** vector  

---

## âš™ï¸ Installation

```bash
# Python 3.9+ recommended
python -m venv .venv
source .venv/bin/activate        # Windows: .venv\Scripts\activate

pip install -r requirements.txt

If you want to use text prompts (--prompt), set your OpenAI key:

export OPENAI_API_KEY=your_key_here


---

ğŸš€ Training

Train a baseline Transformer on UniTraj data:

python experiment.py \
  --train_traj /path/to/train_clean.p \
  --val_traj   /path/to/test_clean.p \
  --out_dir outputs \
  --epochs 30

	â€¢	ğŸ“‚ Checkpoints: outputs/vec_xl_baseline/best.pt, last.pt
	â€¢	ğŸ“Š Results: outputs/results.csv (ADE, FDE, collisions, smoothness)
	â€¢	ğŸ“ˆ Logs: TensorBoard support included

---

ğŸ”® Inference (make GIFs!)

ğŸ“ A) With text prompt (recommended)

python inference.py \
  --checkpoint outputs/vec_xl_baseline/best.pt \
  --traj_file /path/to/test_clean.p \
  --masked_agent 1 \
  --prompt "the center set a screen near the foul line then rolled toward the goal post" \
  --gif_out test.gif \
  --court_png court.png \
  --metrics

ğŸ”¢ B) With explicit region IDs

python inference.py \
  --checkpoint outputs/vec_xl_baseline/best.pt \
  --traj_file /path/to/test_clean.p \
  --masked_agent 1 \
  --region_ids_json "[0,0,1,1,2,2,3,...]" \
  --gif_out test.gif \
  --court_png court.png \
  --metrics

ğŸŸ© C) With Oracle regions (ground truth; for testing)

python inference.py \
  --checkpoint outputs/vec_xl_baseline/best.pt \
  --traj_file /path/to/test_clean.p \
  --masked_agent 1 \
  --gif_out test.gif \
  --court_png court.png \
  --metrics


---

ğŸ“ Data shapes (for techies)

For each sample:
	â€¢	x_in : (T, A*3 + 40)
	â€¢	A*2 â†’ player positions (masked coords zeroed)
	â€¢	A   â†’ observation flags (1=observed, 0=masked)
	â€¢	40  â†’ region one-hot for masked agent
	â€¢	y_gt : (T, A*2)
	â€¢	loss_mask : (T, A*2)

---

ğŸ“Š Metrics
	â€¢	ADE: Average displacement error
	â€¢	FDE: Final displacement error
	â€¢	Collision rate: % of times players overlap unrealistically
	â€¢	Smoothness: Acceleration variance

---

ğŸ TL;DR
	1.	ğŸ“¦ Download datasets from UniTraj-pytorch
	2.	âš™ï¸ Train with experiment.py
	3.	âœï¸ Run inference.py with --prompt "your description"
	4.	ğŸï¸ Check test.gif: green = ground truth, highlighted blue = prediction

---